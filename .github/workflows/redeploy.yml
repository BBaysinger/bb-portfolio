name: Redeploy

on:
  workflow_dispatch:
    inputs:
      environment:
        description: Which profiles to (re)start
        type: choice
        required: true
        default: both
        options: [prod, dev, both]
        # TODO(staging): when ready, extend options to include 'staging' and wire staging secrets below.
      start_dev:
        description: Ensure dev containers are up in addition to prod
        type: choice
        required: true
        default: "true"
        options: ["true", "false"]
      refresh_env:
        description: Regenerate and upload .env files to EC2 (requires secrets)
        type: choice
        required: true
        default: "false"
        options: ["true", "false"]
      restart_containers:
        description: Restart containers on EC2 (docker-compose pull/up)
        type: choice
        required: true
        default: "true"
        options: ["true", "false"]

  workflow_call:
    inputs:
      environment:
        description: Which profiles to (re)start
        type: string
        required: true
        default: both
        # TODO(staging): accept 'staging' here and treat as prod-like with staging-scoped secrets.
      start_dev:
        description: Ensure dev containers are up in addition to prod
        type: string
        required: true
        default: "true"
      refresh_env:
        description: Regenerate and upload .env files to EC2 (requires secrets)
        type: string
        required: true
        default: "false"
      restart_containers:
        description: Restart containers on EC2 (docker-compose pull/up)
        type: string
        required: true
        default: "true"

jobs:
  deploy:
    name: SSH redeploy on EC2
    runs-on: ubuntu-latest
    timeout-minutes: 30
    concurrency:
      group: redeploy-${{ inputs.environment }}
      cancel-in-progress: true
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Prepare SSH key
        id: ssh
        env:
          SSH_KEY_PEM: ${{ secrets.EC2_SSH_KEY_PEM }}
          SSH_KEY_ALT: ${{ secrets.EC2_SSH_KEY }}
        run: |
          set -euo pipefail
          CONTENT="${SSH_KEY_PEM:-}"
          if [ -z "$CONTENT" ] && [ -n "${SSH_KEY_ALT:-}" ]; then
            CONTENT="$SSH_KEY_ALT"
          fi
          if [ -z "$CONTENT" ]; then
            echo "Missing SSH key secret: provide EC2_SSH_KEY_PEM or EC2_SSH_KEY" >&2
            exit 1
          fi
          umask 177
          printf "%s" "$CONTENT" > key.pem
          echo "key=key.pem" >> "$GITHUB_OUTPUT"

      - name: Validate required inputs and secrets
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
          REFRESH_ENV: ${{ inputs.refresh_env }}
          PROD_MONGODB_URI: ${{ secrets.PROD_MONGODB_URI }}
          PROD_PAYLOAD_SECRET: ${{ secrets.PROD_PAYLOAD_SECRET }}
          PROD_S3_BUCKET: ${{ secrets.PROD_S3_BUCKET }}
          PROD_AWS_REGION: ${{ secrets.PROD_AWS_REGION }}
          DEV_MONGODB_URI: ${{ secrets.DEV_MONGODB_URI }}
          DEV_PAYLOAD_SECRET: ${{ secrets.DEV_PAYLOAD_SECRET }}
          DEV_S3_BUCKET: ${{ secrets.DEV_S3_BUCKET }}
          DEV_AWS_REGION: ${{ secrets.DEV_AWS_REGION }}
        run: |
          set -euo pipefail
          : "${ENVIRONMENT}"
          : "${EC2_HOST}"
          : "${REFRESH_ENV}"
          # Only enforce env-specific secrets for the selected environment(s)
          if [ "$REFRESH_ENV" = "true" ]; then
            case "$ENVIRONMENT" in
              prod)
                : "${PROD_MONGODB_URI}"
                : "${PROD_PAYLOAD_SECRET}"
                : "${PROD_S3_BUCKET}"
                : "${PROD_AWS_REGION}"
                ;;
              dev)
                : "${DEV_MONGODB_URI}"
                : "${DEV_PAYLOAD_SECRET}"
                : "${DEV_S3_BUCKET}"
                : "${DEV_AWS_REGION}"
                ;;
              both)
                : "${PROD_MONGODB_URI}"
                : "${PROD_PAYLOAD_SECRET}"
                : "${PROD_S3_BUCKET}"
                : "${PROD_AWS_REGION}"
                : "${DEV_MONGODB_URI}"
                : "${DEV_PAYLOAD_SECRET}"
                : "${DEV_S3_BUCKET}"
                : "${DEV_AWS_REGION}"
                ;;
            esac
          fi

      - name: Generate env files (in runner temp)
        id: genenv
        if: ${{ inputs.refresh_env == 'true' || inputs.refresh_env == true }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          # Common
          S3_REGION: ${{ secrets.S3_REGION }}
          # Prod
          PROD_MONGODB_URI: ${{ secrets.PROD_MONGODB_URI }}
          PROD_PAYLOAD_SECRET: ${{ secrets.PROD_PAYLOAD_SECRET }}
          PROD_S3_BUCKET: ${{ secrets.PROD_S3_BUCKET }}
          PROD_AWS_REGION: ${{ secrets.PROD_AWS_REGION }}
          PROD_FRONTEND_URL: ${{ secrets.PROD_FRONTEND_URL }}
          PROD_NEXT_PUBLIC_BACKEND_URL: ${{ secrets.PROD_NEXT_PUBLIC_BACKEND_URL }}
          PROD_BACKEND_INTERNAL_URL: ${{ secrets.PROD_BACKEND_INTERNAL_URL }}
          PROD_SES_FROM_EMAIL: ${{ secrets.PROD_SES_FROM_EMAIL }}
          PROD_SES_TO_EMAIL: ${{ secrets.PROD_SES_TO_EMAIL }}
          # Dev
          DEV_MONGODB_URI: ${{ secrets.DEV_MONGODB_URI }}
          DEV_PAYLOAD_SECRET: ${{ secrets.DEV_PAYLOAD_SECRET }}
          DEV_S3_BUCKET: ${{ secrets.DEV_S3_BUCKET }}
          DEV_AWS_REGION: ${{ secrets.DEV_AWS_REGION }}
          DEV_FRONTEND_URL: ${{ secrets.DEV_FRONTEND_URL }}
          DEV_NEXT_PUBLIC_BACKEND_URL: ${{ secrets.DEV_NEXT_PUBLIC_BACKEND_URL }}
          DEV_BACKEND_INTERNAL_URL: ${{ secrets.DEV_BACKEND_INTERNAL_URL }}
          DEV_SES_FROM_EMAIL: ${{ secrets.DEV_SES_FROM_EMAIL }}
          DEV_SES_TO_EMAIL: ${{ secrets.DEV_SES_TO_EMAIL }}
        run: |
          set -euo pipefail
          OUT_DIR="$(mktemp -d)"

          s_val() { [ -n "${!1:-}" ] && echo "${!1}" || echo "${2:-}"; }

          # Write files using printf to avoid heredoc/YAML indentation pitfalls
          printf "%s\n" \
            "NODE_ENV=production" \
            "ENV_PROFILE=prod" \
            "" \
            "PROD_AWS_REGION=$(s_val PROD_AWS_REGION \"$S3_REGION\")" \
            "" \
            "PROD_MONGODB_URI=$(s_val PROD_MONGODB_URI)" \
            "PROD_PAYLOAD_SECRET=$(s_val PROD_PAYLOAD_SECRET)" \
            "" \
            "PROD_S3_BUCKET=$(s_val PROD_S3_BUCKET)" \
            "S3_REGION=$(s_val S3_REGION \"$PROD_AWS_REGION\")" \
            "" \
            "PROD_FRONTEND_URL=$(s_val PROD_FRONTEND_URL)" \
            "PROD_NEXT_PUBLIC_BACKEND_URL=$(s_val PROD_NEXT_PUBLIC_BACKEND_URL)" \
            "PROD_BACKEND_INTERNAL_URL=$(s_val PROD_BACKEND_INTERNAL_URL 'http://bb-portfolio-backend-prod:3000')" \
            "" \
            "PROD_SES_FROM_EMAIL=$(s_val PROD_SES_FROM_EMAIL)" \
            "PROD_SES_TO_EMAIL=$(s_val PROD_SES_TO_EMAIL)" \
            > "$OUT_DIR/backend.env.prod"

          printf "%s\n" \
            "NODE_ENV=development" \
            "ENV_PROFILE=dev" \
            "PORT=3000" \
            "" \
            "DEV_AWS_REGION=$(s_val DEV_AWS_REGION \"$S3_REGION\")" \
            "" \
            "DEV_MONGODB_URI=$(s_val DEV_MONGODB_URI)" \
            "DEV_PAYLOAD_SECRET=$(s_val DEV_PAYLOAD_SECRET)" \
            "" \
            "DEV_S3_BUCKET=$(s_val DEV_S3_BUCKET)" \
            "S3_REGION=$(s_val S3_REGION \"$DEV_AWS_REGION\")" \
            "" \
            "DEV_FRONTEND_URL=$(s_val DEV_FRONTEND_URL)" \
            "DEV_NEXT_PUBLIC_BACKEND_URL=$(s_val DEV_NEXT_PUBLIC_BACKEND_URL)" \
            "DEV_BACKEND_INTERNAL_URL=$(s_val DEV_BACKEND_INTERNAL_URL 'http://bb-portfolio-backend-dev:3000')" \
            "" \
            "DEV_SES_FROM_EMAIL=$(s_val DEV_SES_FROM_EMAIL)" \
            "DEV_SES_TO_EMAIL=$(s_val DEV_SES_TO_EMAIL)" \
            > "$OUT_DIR/backend.env.dev"

          printf "%s\n" \
            "NODE_ENV=production" \
            "ENV_PROFILE=prod" \
            "" \
            "PROD_BACKEND_INTERNAL_URL=$(s_val PROD_BACKEND_INTERNAL_URL 'http://bb-portfolio-backend-prod:3000')" \
            "NEXT_PUBLIC_BACKEND_URL=$(s_val PROD_NEXT_PUBLIC_BACKEND_URL)" \
            > "$OUT_DIR/frontend.env.prod"

          printf "%s\n" \
            "NODE_ENV=development" \
            "ENV_PROFILE=dev" \
            "" \
            "DEV_BACKEND_INTERNAL_URL=$(s_val DEV_BACKEND_INTERNAL_URL 'http://bb-portfolio-backend-dev:3000')" \
            "NEXT_PUBLIC_BACKEND_URL=$(s_val DEV_NEXT_PUBLIC_BACKEND_URL)" \
            > "$OUT_DIR/frontend.env.dev"

          echo "dir=$OUT_DIR" >> "$GITHUB_OUTPUT"

      - name: Upload env files to EC2
        if: ${{ inputs.refresh_env == 'true' || inputs.refresh_env == true }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          OUT_DIR='${{ steps.genenv.outputs.dir }}'
          test -n "$OUT_DIR" && test -d "$OUT_DIR"

          # Prepare remote directories and ensure correct ownership (idempotent)
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" "sudo mkdir -p /home/ec2-user/portfolio/backend /home/ec2-user/portfolio/frontend && sudo chown -R ec2-user:ec2-user /home/ec2-user/portfolio"

          scp -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            "$OUT_DIR/backend.env.prod"  ec2-user@"$EC2_HOST":/home/ec2-user/portfolio/backend/.env.prod
          scp -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            "$OUT_DIR/backend.env.dev"   ec2-user@"$EC2_HOST":/home/ec2-user/portfolio/backend/.env.dev
          scp -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            "$OUT_DIR/frontend.env.prod" ec2-user@"$EC2_HOST":/home/ec2-user/portfolio/frontend/.env.prod
          scp -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            "$OUT_DIR/frontend.env.dev"  ec2-user@"$EC2_HOST":/home/ec2-user/portfolio/frontend/.env.dev

      - name: Upload compose file to EC2
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          # Ensure target directory exists on EC2
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" "sudo mkdir -p /home/ec2-user/portfolio/deploy/compose && sudo chown -R ec2-user:ec2-user /home/ec2-user/portfolio"
          # Upload the compose file from repo to the expected remote path
          scp -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            "deploy/compose/docker-compose.yml" \
            ec2-user@"$EC2_HOST":/home/ec2-user/portfolio/deploy/compose/docker-compose.yml

      - name: Restart containers
        if: ${{ inputs.restart_containers == 'true' || inputs.restart_containers == true }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          # Pre-clean on EC2 to prevent disk-full and name conflicts
          {
            printf '%s\n' 'set -e'
            printf '%s\n' 'echo "== Disk usage before =="'
            printf '%s\n' 'df -h / || true'
            printf '%s\n' 'docker system df || true'
            printf '%s\n' 'echo "== Stop/remove any stale containers (ignore errors) =="'
            printf '%s\n' 'docker rm -f bb-portfolio-backend-dev bb-portfolio-frontend-dev bb-portfolio-backend-prod bb-portfolio-frontend-prod 2>/dev/null || true'
            printf '%s\n' 'echo "== Prune unused images/containers/networks (no volumes) =="'
            printf '%s\n' 'docker system prune -af || true'
            printf '%s\n' 'echo "== Prune builder cache =="'
            printf '%s\n' 'docker builder prune -af || true'
            printf '%s\n' 'echo "== Check free space and prune volumes only if critically low =="'
            printf '%s\n' 'FREE_KB=$(df -Pk / | awk "NR==2{print $4}")'
            printf '%s\n' 'THRESHOLD_KB=$((3 * 1024 * 1024)) # 3GB'
            printf '%s\n' 'if [ "$FREE_KB" -lt "$THRESHOLD_KB" ]; then'
            printf '%s\n' '  echo "Low disk space detected ($(df -h / | awk \"NR==2{print \$4}\")) — pruning unused volumes..."'
            printf '%s\n' '  docker volume prune -f || true'
            printf '%s\n' 'fi'
            printf '%s\n' 'echo "== Disk usage after prune =="'
            printf '%s\n' 'df -h / || true'
            printf '%s\n' 'docker system df || true'
          } | ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" "bash -s"

          # Now perform the actual restart using docker-compose
          {
            printf '%s\n' 'set -e'
            printf '%s\n' 'cd /home/ec2-user/portfolio'
            printf '%s\n' 'aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 778230822028.dkr.ecr.us-west-2.amazonaws.com >/dev/null 2>&1 || true'
            printf '%s\n' 'export AWS_ACCOUNT_ID=778230822028'
            printf '%s\n' 'COMPOSE_FILE="deploy/compose/docker-compose.yml"'
            # Avoid full down to reduce downtime; rely on up --force-recreate per profile
            printf '%s\n' '# docker-compose -f "$COMPOSE_FILE" down --remove-orphans || true'
            printf '%s\n' ''
            printf '%s\n' 'case "${ENVIRONMENT}" in'
            printf '%s\n' '  prod)'
            printf '%s\n' '    COMPOSE_PROFILES=prod docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    COMPOSE_PROFILES=prod docker-compose -f "$COMPOSE_FILE" up -d --force-recreate'
            printf '%s\n' '    ;;'
            printf '%s\n' '  dev)'
            printf '%s\n' '    COMPOSE_PROFILES=dev docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    COMPOSE_PROFILES=dev docker-compose -f "$COMPOSE_FILE" up -d --force-recreate'
            printf '%s\n' '    ;;'
            printf '%s\n' '  both)'
            printf '%s\n' '    COMPOSE_PROFILES=prod docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    COMPOSE_PROFILES=prod docker-compose -f "$COMPOSE_FILE" up -d --force-recreate || true'
            printf '%s\n' '    COMPOSE_PROFILES=dev docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    COMPOSE_PROFILES=dev docker-compose -f "$COMPOSE_FILE" up -d --force-recreate || true'
            printf '%s\n' '    ;;'
            printf '%s\n' 'esac'
          } | ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" "ENVIRONMENT='${ENVIRONMENT}' bash -s"

      - name: Diagnose services (containers and nginx)
        if: always()
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" $'set -e
            echo "== Docker ps =="
            docker ps --format "table {{.Names}}\t{{.Image}}\t{{.Status}}\t{{.Ports}}"
            echo
            echo "== Listening ports (ss) =="
            ss -lntp || true
            echo
            echo "== Compose ps (prod) =="
            COMPOSE_PROFILES=prod docker-compose -f /home/ec2-user/portfolio/deploy/compose/docker-compose.yml ps || true
            echo
            echo "== Nginx status =="
            systemctl is-active nginx || true
            nginx -t || true
            echo
            echo "== HTTP probes =="
            for url in \
              "http://localhost:3000/" \
              "http://localhost:3001/api/health" \
              "http://localhost:4000/" \
              "http://localhost:4001/api/health"; do
              code=$(curl -s -o /dev/null -w "%{http_code}" "$url" || true)
              echo "$code $url"
            done
          '

      - name: Health checks
        if: always()
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          EC2_HOST: ${{ secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          check() {
            ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" \
              "curl -s -o /dev/null -w '%{http_code}' http://localhost:$1$2"
          }
          ok_code() {
            case "$1" in
              2*|3*) return 0;;
              *) return 1;;
            esac
          }
          if [ "$ENVIRONMENT" = "prod" ] || [ "$ENVIRONMENT" = "both" ]; then
            code=$(check 3000 /) && ok_code "$code" || echo "WARN: frontend prod HTTP $code" >&2
            code=$(check 3001 /api/health) && ok_code "$code" || echo "WARN: backend prod health HTTP $code" >&2
          fi
          if [ "$ENVIRONMENT" = "dev" ] || [ "$ENVIRONMENT" = "both" ]; then
            code=$(check 4000 /) && ok_code "$code" || echo "WARN: frontend dev HTTP $code" >&2
            code=$(check 4001 /api/health) && ok_code "$code" || echo "WARN: backend dev health HTTP $code" >&2
          fi
