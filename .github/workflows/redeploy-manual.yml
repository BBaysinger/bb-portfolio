name: Redeploy (Manual)

# IP Address Constants for Blue-Green-Red Deployment
# GREEN (Production/Active): 44.246.43.116
# BLUE (Candidate/Staging): 52.37.142.50
# RED (Tainted/Recreation): 35.167.120.233

on:
  workflow_dispatch:
    inputs:
      environment:
        description: Which profiles to (re)start
        type: choice
        required: true
        default: both
        options: [prod, dev, both]
      ec2_host:
        description: Optional override EC2 host/IP for SSH
        type: string
        required: false
        default: ""
      start_dev:
        description: Ensure dev containers are up in addition to prod
        type: choice
        required: true
        default: "true"
        options: ["true", "false"]
      refresh_env:
        description: Regenerate and upload .env files to EC2 (requires secrets)
        type: choice
        required: true
        default: "false"
        options: ["true", "false"]
      restart_containers:
        description: Restart containers on EC2 (docker-compose pull/up)
        type: choice
        required: true
        default: "true"
        options: ["true", "false"]

permissions:
  contents: read
  actions: read

jobs:
  sanity:
    runs-on: ubuntu-latest
    steps:
      - name: Print inputs
        run: |
          echo "environment=${{ inputs.environment }}"
          echo "start_dev=${{ inputs.start_dev }}"
          echo "refresh_env=${{ inputs.refresh_env }}"
          echo "restart_containers=${{ inputs.restart_containers }}"
  deploy:
    name: SSH redeploy on EC2 (manual)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    concurrency:
      group: redeploy-${{ inputs.environment == 'both' && 'global' || inputs.environment }}
      cancel-in-progress: true
    steps:
      - name: Early conflict guard
        uses: actions/github-script@v7
        env:
          ENVIRONMENT: ${{ inputs.environment }}
        with:
          script: |
            const owner = context.repo.owner;
            const repo = context.repo.repo;
            const envFilter = process.env.ENVIRONMENT; // prod | dev | both
            async function list(workflow, status){
              try {
                return (await github.rest.actions.listWorkflowRuns({ owner, repo, workflow_id: workflow, status })).data.workflow_runs;
              } catch (e) { return []; }
            }
            const statuses = ['in_progress','queued'];
            const workflows = ['redeploy.yml']; // auto deploy workflow file name
            let conflicts = [];
            for (const wf of workflows) {
              for (const st of statuses) {
                const runs = await list(wf, st);
                // If environment == both we treat any run as a conflict; otherwise allow filtering logic if future metadata added
                conflicts.push(...runs);
              }
            }
            if (conflicts.length) {
              core.setFailed(`Conflicting in-progress or queued auto redeploy(s) detected: ${conflicts.map(r => r.id).join(', ')}`);
            } else {
              core.info('No conflicting auto redeploy runs detected');
            }

      - name: Environment secrets guard
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
          REFRESH_ENV: ${{ inputs.refresh_env }}
          PROD_MONGODB_URI: ${{ secrets.PROD_MONGODB_URI }}
          PROD_PAYLOAD_SECRET: ${{ secrets.PROD_PAYLOAD_SECRET }}
          PROD_S3_BUCKET: ${{ secrets.PROD_S3_BUCKET }}
          PROD_AWS_REGION: ${{ secrets.PROD_AWS_REGION }}
          DEV_MONGODB_URI: ${{ secrets.DEV_MONGODB_URI }}
          DEV_PAYLOAD_SECRET: ${{ secrets.DEV_PAYLOAD_SECRET }}
          DEV_S3_BUCKET: ${{ secrets.DEV_S3_BUCKET }}
          DEV_AWS_REGION: ${{ secrets.DEV_AWS_REGION }}
        run: |
          set -euo pipefail
          : "${ENVIRONMENT}"
          : "${EC2_HOST}"
          : "${REFRESH_ENV}"
          if [ "$REFRESH_ENV" = "true" ]; then
            case "$ENVIRONMENT" in
              prod)
                : "${PROD_MONGODB_URI}" "PROD_MONGODB_URI missing"
                : "${PROD_PAYLOAD_SECRET}" "PROD_PAYLOAD_SECRET missing"
                : "${PROD_S3_BUCKET}" "PROD_S3_BUCKET missing"
                : "${PROD_AWS_REGION}" "PROD_AWS_REGION missing"
                ;;
              dev)
                : "${DEV_MONGODB_URI}" "DEV_MONGODB_URI missing"
                : "${DEV_PAYLOAD_SECRET}" "DEV_PAYLOAD_SECRET missing"
                : "${DEV_S3_BUCKET}" "DEV_S3_BUCKET missing"
                : "${DEV_AWS_REGION}" "DEV_AWS_REGION missing"
                ;;
              both)
                : "${PROD_MONGODB_URI}" "PROD_MONGODB_URI missing"
                : "${PROD_PAYLOAD_SECRET}" "PROD_PAYLOAD_SECRET missing"
                : "${PROD_S3_BUCKET}" "PROD_S3_BUCKET missing"
                : "${PROD_AWS_REGION}" "PROD_AWS_REGION missing"
                : "${DEV_MONGODB_URI}" "DEV_MONGODB_URI missing"
                : "${DEV_PAYLOAD_SECRET}" "DEV_PAYLOAD_SECRET missing"
                : "${DEV_S3_BUCKET}" "DEV_S3_BUCKET missing"
                : "${DEV_AWS_REGION}" "DEV_AWS_REGION missing"
                ;;
            esac
          fi

      - name: SSH preflight (enhanced diagnostics)
        id: ssh_preflight
        env:
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          echo "=== Enhanced SSH Preflight Diagnostics ==="
          echo "Target: $EC2_HOST"
          echo ""

          # 1. Network connectivity (port 22)
          echo "1. Testing port 22 connectivity with netcat..."
          if timeout 5 bash -c "echo > /dev/tcp/$EC2_HOST/22" 2>/dev/null; then
            echo "✓ Port 22 is open and accepting connections"
          else
            echo "✗ Port 22 appears closed or unreachable"
            echo "This suggests either:"
            echo "  - Security group blocking port 22"
            echo "  - Instance not running"
            echo "  - Wrong IP address"
            exit 1
          fi
          echo ""

          # 2. Attempt SSH with multiple common usernames
          echo "2. Attempting SSH with common usernames..."
          SSH_OPTS="-o BatchMode=yes -o ConnectTimeout=5 -o ServerAliveInterval=2 -o ServerAliveCountMax=2 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i ${{ steps.ssh.outputs.key }}"
          USERS="ec2-user ubuntu admin"
          SSH_SUCCESS=false
          WORKING_USER=""

          for user in $USERS; do
            echo "  Trying: $user@$EC2_HOST"
            if timeout 8 ssh $SSH_OPTS ${user}@"$EC2_HOST" "echo 'SSH_TEST_SUCCESS' && uptime" 2>/dev/null | grep -q "SSH_TEST_SUCCESS"; then
              echo "  ✓ Success with user: $user"
              SSH_SUCCESS=true
              WORKING_USER="$user"
              break
            else
              echo "  ✗ Failed with user: $user"
            fi
          done
          echo ""

          if [ "$SSH_SUCCESS" = "false" ]; then
            echo "=== SSH Preflight FAILED ==="
            echo "Port 22 is open but SSH authentication failed for all users: $USERS"
            echo "Possible causes:"
            echo "  - Wrong SSH key (key in GitHub secret doesn't match instance authorized_keys)"
            echo "  - Instance was replaced/rebuilt and key not re-added"
            echo "  - Permissions issue with authorized_keys on instance"
            echo "  - Different AMI with non-standard username"
            exit 1
          fi

          echo "=== SSH Preflight SUCCESS ==="
          echo "Working user: $WORKING_USER"
          echo "ssh_user=$WORKING_USER" >> "$GITHUB_OUTPUT"

      - name: "Guard: env files present when refresh_env=false"
        if: ${{ inputs.refresh_env == 'false' || inputs.refresh_env == false }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
          SSH_USER: ${{ steps.ssh_preflight.outputs.ssh_user }}
          ENVIRONMENT: ${{ inputs.environment }}
        run: |
          set -euo pipefail
          USER="${SSH_USER:-ec2-user}"
          KEY="${{ steps.ssh.outputs.key }}"
          missing=0
          check() {
            local f="$1"
            if ! ssh -i "$KEY" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null "${USER}@$EC2_HOST" "test -f '$f'"; then
              echo "✗ Missing $f"
              missing=1
            else
              echo "✓ Found  $f"
            fi
          }
          case "$ENVIRONMENT" in
            prod)
              check "/home/${USER}/bb-portfolio/backend/.env.prod"
              check "/home/${USER}/bb-portfolio/frontend/.env.prod"
              ;;
            dev)
              check "/home/${USER}/bb-portfolio/backend/.env.dev"
              check "/home/${USER}/bb-portfolio/frontend/.env.dev"
              ;;
            both)
              check "/home/${USER}/bb-portfolio/backend/.env.prod"
              check "/home/${USER}/bb-portfolio/frontend/.env.prod"
              check "/home/${USER}/bb-portfolio/backend/.env.dev"
              check "/home/${USER}/bb-portfolio/frontend/.env.dev"
              ;;
          esac
          if [ "$missing" -ne 0 ]; then
            echo "One or more required env files are missing on the host."
            echo "Re-run this workflow with input refresh_env=true to generate and upload them."
            exit 1
          fi

      - name: Generate env files (in runner temp)
        id: genenv
        if: ${{ inputs.refresh_env == 'true' || inputs.refresh_env == true }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          # Common
          S3_REGION: ${{ secrets.S3_REGION }}
          PUBLIC_PROJECTS_BUCKET: ${{ secrets.PUBLIC_PROJECTS_BUCKET }}
          NDA_PROJECTS_BUCKET: ${{ secrets.NDA_PROJECTS_BUCKET }}
          SECURITY_TXT_EXPIRES: ${{ secrets.SECURITY_TXT_EXPIRES }}
          # Unified env-guard definition lists (per-profile)
          PROD_REQUIRED_ENVIRONMENT_VARIABLES: ${{ secrets.PROD_REQUIRED_ENVIRONMENT_VARIABLES }}
          DEV_REQUIRED_ENVIRONMENT_VARIABLES: ${{ secrets.DEV_REQUIRED_ENVIRONMENT_VARIABLES }}
          # Prod
          PROD_MONGODB_URI: ${{ secrets.PROD_MONGODB_URI }}
          PROD_PAYLOAD_SECRET: ${{ secrets.PROD_PAYLOAD_SECRET }}
          PROD_S3_BUCKET: ${{ secrets.PROD_S3_BUCKET }}
          PROD_AWS_REGION: ${{ secrets.PROD_AWS_REGION }}
          PROD_FRONTEND_URL: ${{ secrets.PROD_FRONTEND_URL }}
          PROD_BACKEND_INTERNAL_URL: ${{ secrets.PROD_BACKEND_INTERNAL_URL }}
          PROD_SES_FROM_EMAIL: ${{ secrets.PROD_SES_FROM_EMAIL }}
          PROD_SES_TO_EMAIL: ${{ secrets.PROD_SES_TO_EMAIL }}
          # Dev
          DEV_MONGODB_URI: ${{ secrets.DEV_MONGODB_URI }}
          DEV_PAYLOAD_SECRET: ${{ secrets.DEV_PAYLOAD_SECRET }}
          DEV_S3_BUCKET: ${{ secrets.DEV_S3_BUCKET }}
          DEV_AWS_REGION: ${{ secrets.DEV_AWS_REGION }}
          DEV_FRONTEND_URL: ${{ secrets.DEV_FRONTEND_URL }}
          DEV_BACKEND_INTERNAL_URL: ${{ secrets.DEV_BACKEND_INTERNAL_URL }}
          DEV_SES_FROM_EMAIL: ${{ secrets.DEV_SES_FROM_EMAIL }}
          DEV_SES_TO_EMAIL: ${{ secrets.DEV_SES_TO_EMAIL }}
        run: |
          set -euo pipefail
          OUT_DIR="$(mktemp -d)"

          s_val() { [ -n "${!1:-}" ] && echo "${!1}" || echo "${2:-}"; }

          # Write files using printf to avoid heredoc/YAML indentation pitfalls
          printf "%s\n" \
            "NODE_ENV=production" \
            "ENV_PROFILE=prod" \
            "" \
            "# Env-guard definition list for prod profile" \
            "PROD_REQUIRED_ENVIRONMENT_VARIABLES=$(s_val PROD_REQUIRED_ENVIRONMENT_VARIABLES)" \
            "" \
            "PROD_AWS_REGION=$(s_val PROD_AWS_REGION \"$S3_REGION\")" \
            "" \
            "PROD_MONGODB_URI=$(s_val PROD_MONGODB_URI)" \
            "PROD_PAYLOAD_SECRET=$(s_val PROD_PAYLOAD_SECRET)" \
            "" \
            "PROD_S3_BUCKET=$(s_val PROD_S3_BUCKET)" \
            "PUBLIC_PROJECTS_BUCKET=$(s_val PUBLIC_PROJECTS_BUCKET)" \
            "NDA_PROJECTS_BUCKET=$(s_val NDA_PROJECTS_BUCKET)" \
            "S3_REGION=$(s_val S3_REGION \"$PROD_AWS_REGION\")" \
            "" \
            "PROD_FRONTEND_URL=$(s_val PROD_FRONTEND_URL)" \
            "PROD_BACKEND_INTERNAL_URL=$(s_val PROD_BACKEND_INTERNAL_URL 'http://bb-portfolio-backend-prod:3000')" \
            "" \
            "SECURITY_TXT_EXPIRES=$(s_val SECURITY_TXT_EXPIRES)" \
            "" \
            "PROD_SES_FROM_EMAIL=$(s_val PROD_SES_FROM_EMAIL)" \
            "PROD_SES_TO_EMAIL=$(s_val PROD_SES_TO_EMAIL)" \
            > "$OUT_DIR/backend.env.prod"

          printf "%s\n" \
            "NODE_ENV=development" \
            "ENV_PROFILE=dev" \
            "" \
            "# Env-guard definition list for dev profile" \
            "DEV_REQUIRED_ENVIRONMENT_VARIABLES=$(s_val DEV_REQUIRED_ENVIRONMENT_VARIABLES)" \
            "" \
            "DEV_AWS_REGION=$(s_val DEV_AWS_REGION \"$S3_REGION\")" \
            "" \
            "DEV_MONGODB_URI=$(s_val DEV_MONGODB_URI)" \
            "DEV_PAYLOAD_SECRET=$(s_val DEV_PAYLOAD_SECRET)" \
            "" \
            "DEV_S3_BUCKET=$(s_val DEV_S3_BUCKET)" \
            "PUBLIC_PROJECTS_BUCKET=$(s_val PUBLIC_PROJECTS_BUCKET)" \
            "NDA_PROJECTS_BUCKET=$(s_val NDA_PROJECTS_BUCKET)" \
            "S3_REGION=$(s_val S3_REGION \"$DEV_AWS_REGION\")" \
            "" \
            "DEV_FRONTEND_URL=$(s_val DEV_FRONTEND_URL)" \
            "DEV_BACKEND_INTERNAL_URL=$(s_val DEV_BACKEND_INTERNAL_URL 'http://bb-portfolio-backend-dev:3000')" \
            "" \
            "SECURITY_TXT_EXPIRES=$(s_val SECURITY_TXT_EXPIRES)" \
            "" \
            "DEV_SES_FROM_EMAIL=$(s_val DEV_SES_FROM_EMAIL)" \
            "DEV_SES_TO_EMAIL=$(s_val DEV_SES_TO_EMAIL)" \
            > "$OUT_DIR/backend.env.dev"

          printf "%s\n" \
            "NODE_ENV=production" \
            "ENV_PROFILE=prod" \
            "" \
            "PUBLIC_PROJECTS_BUCKET=$(s_val PUBLIC_PROJECTS_BUCKET)" \
            "NDA_PROJECTS_BUCKET=$(s_val NDA_PROJECTS_BUCKET)" \
            > "$OUT_DIR/frontend.env.prod"

          printf "%s\n" \
            "NODE_ENV=development" \
            "ENV_PROFILE=dev" \
            "" \
            "PUBLIC_PROJECTS_BUCKET=$(s_val PUBLIC_PROJECTS_BUCKET)" \
            "NDA_PROJECTS_BUCKET=$(s_val NDA_PROJECTS_BUCKET)" \
            > "$OUT_DIR/frontend.env.dev"

          echo "dir=$OUT_DIR" >> "$GITHUB_OUTPUT"

      - name: Upload env files to EC2
        if: ${{ inputs.refresh_env == 'true' || inputs.refresh_env == true }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
          SSH_USER: ${{ steps.ssh_preflight.outputs.ssh_user }}
          SSH_OPTS: -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10 -o ServerAliveInterval=5 -o ServerAliveCountMax=2 -o PreferredAuthentications=publickey -o PubkeyAuthentication=yes -o TCPKeepAlive=yes -o Compression=yes
        run: |
          set -euo pipefail
          OUT_DIR='${{ steps.genenv.outputs.dir }}'
          test -n "$OUT_DIR" && test -d "$OUT_DIR"
          USER="${SSH_USER:-ec2-user}"
          echo "== Packaging env files for single transfer =="
          tar -C "$OUT_DIR" -czf env-bundle.tgz backend.env.prod backend.env.dev frontend.env.prod frontend.env.dev
          echo "Bundle size:" $(du -h env-bundle.tgz | cut -f1)
          # Prepare remote directories
          ssh -i "${{ steps.ssh.outputs.key }}" $SSH_OPTS "${USER}@$EC2_HOST" "mkdir -p /home/${USER}/bb-portfolio/{backend,frontend}"
          upload_attempts=0
          until scp -i "${{ steps.ssh.outputs.key }}" $SSH_OPTS env-bundle.tgz "${USER}@$EC2_HOST:/home/${USER}/bb-portfolio/env-bundle.tgz"; do
            upload_attempts=$((upload_attempts+1))
            if [ $upload_attempts -ge 3 ]; then
              echo "Env bundle upload failed after $upload_attempts attempts" >&2
              exit 1
            fi
            echo "Upload attempt $upload_attempts failed; retrying in 4s..." >&2
            sleep 4
          done
          ssh -i "${{ steps.ssh.outputs.key }}" $SSH_OPTS "${USER}@$EC2_HOST" $'set -e
            cd /home/'"${USER}"'/bb-portfolio
            tar -xzf env-bundle.tgz || { echo "Failed to extract env bundle" >&2; exit 1; }
            mv -f backend.env.prod backend/.env.prod
            mv -f backend.env.dev backend/.env.dev
            mv -f frontend.env.prod frontend/.env.prod
            mv -f frontend.env.dev frontend/.env.dev
            rm -f env-bundle.tgz
            echo "Env files deployed:"; ls -l backend/.env.* frontend/.env.*
          '
          echo "== Env upload complete =="

      - name: Restart containers
        if: ${{ inputs.restart_containers == 'true' || inputs.restart_containers == true }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
          SSH_USER: ${{ steps.ssh_preflight.outputs.ssh_user }}
        run: |
          set -euo pipefail
          USER="${SSH_USER:-ec2-user}"
          {
            printf '%s\n' 'set -e'
            printf '%s\n' "cd /home/${USER}/bb-portfolio"
            printf '%s\n' 'aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 778230822028.dkr.ecr.us-west-2.amazonaws.com >/dev/null 2>&1 || true'
            printf '%s\n' 'export AWS_ACCOUNT_ID=778230822028'
            printf '%s\n' 'COMPOSE_FILE="deploy/compose/docker-compose.yml"'
            # Avoid full down to reduce downtime; rely on up --force-recreate per profile
            printf '%s\n' '# docker-compose down || true'
            printf '%s\n' ''
            # Retry helper to tolerate transient "removal in progress" errors from Docker
            printf '%s\n' 'try_up() {'
            printf '%s\n' '  local profile="$1"'
            printf '%s\n' '  local attempts=0'
            printf '%s\n' '  until [ $attempts -ge 3 ]; do'
            printf '%s\n' '    if COMPOSE_PROFILES="$profile" docker-compose -f "$COMPOSE_FILE" up -d --force-recreate; then'
            printf '%s\n' '      return 0'
            printf '%s\n' '    fi'
            printf '%s\n' '    attempts=$((attempts+1))'
            printf '%s\n' '    echo "compose up failed for $profile (attempt $attempts), retrying in 5s..."'
            printf '%s\n' '    sleep 5'
            printf '%s\n' '  done'
            printf '%s\n' '  echo "compose up failed for $profile after retries" >&2'
            printf '%s\n' '  return 1'
            printf '%s\n' '}'
            printf '%s\n' 'ENVIRONMENT="${ENVIRONMENT:-both}"'
            printf '%s\n' 'case "${ENVIRONMENT}" in'
            printf '%s\n' '  prod)'
            printf '%s\n' '    COMPOSE_PROFILES=prod docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    try_up prod'
            printf '%s\n' '    ;;'
            printf '%s\n' '  dev)'
            printf '%s\n' '    COMPOSE_PROFILES=dev docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    try_up dev'
            printf '%s\n' '    ;;'
            printf '%s\n' '  both)'
            printf '%s\n' '    COMPOSE_PROFILES=prod docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    try_up prod || true'
            printf '%s\n' '    COMPOSE_PROFILES=dev docker-compose -f "$COMPOSE_FILE" pull || true'
            printf '%s\n' '    try_up dev || true'
            printf '%s\n' '    ;;'
            printf '%s\n' 'esac'
          } | ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null "${USER}@$EC2_HOST" "ENVIRONMENT='${ENVIRONMENT}' bash -s"

      - name: Sync Nginx config and reload
        env:
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
          SSH_USER: ${{ steps.ssh_preflight.outputs.ssh_user }}
        run: |
          set -euo pipefail
          USER="${SSH_USER:-ec2-user}"
          echo "== Uploading Nginx vhost config =="
          scp -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -C \
            deploy/nginx/bb-portfolio.conf "${USER}@$EC2_HOST:/home/${USER}/bb-portfolio/bb-portfolio.conf"
          echo "== Installing to /etc/nginx/conf.d and reloading =="
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null "${USER}@$EC2_HOST" $'set -e
            sudo mkdir -p /etc/nginx/conf.d
            sudo mv -f /home/'"${USER}"'/bb-portfolio/bb-portfolio.conf /etc/nginx/conf.d/bb-portfolio.conf
            sudo chown root:root /etc/nginx/conf.d/bb-portfolio.conf
            sudo nginx -t
            sudo systemctl reload nginx
            echo "Nginx active: $(systemctl is-active nginx)"
            echo "Listeners:"; sudo ss -ltnp | egrep ":80|:443" || true
            echo "HTTP /healthz:"; curl -s -I --max-time 4 http://127.0.0.1/healthz || true
          '

      - name: Health checks
        if: ${{ success() }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          START_DEV: ${{ inputs.start_dev }}
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
          SSH_USER: ${{ steps.ssh_preflight.outputs.ssh_user }}
        run: |
          set -euo pipefail
          USER="${SSH_USER:-ec2-user}"
          check() { ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null "${USER}@$EC2_HOST" "curl --max-time 3 --connect-timeout 1 -s -o /dev/null -w '%{http_code}' http://localhost:$1$2"; }
          if [ "$ENVIRONMENT" = "prod" ] || [ "$ENVIRONMENT" = "both" ]; then
            test "$(check 3000 /)" = "200" || echo "WARN: frontend prod not 200" >&2
            test "$(check 3001 /api/health)" = "200" || echo "WARN: backend prod health not 200" >&2
          fi
          if { [ "$ENVIRONMENT" = "dev" ] || [ "$ENVIRONMENT" = "both" ]; } && { [ "${START_DEV}" = "true" ] || [ "${START_DEV}" = true ]; }; then
            test "$(check 4000 /)" = "200" || echo "WARN: frontend dev not 200" >&2
            test "$(check 4001 /api/health)" = "200" || echo "WARN: backend dev health not 200" >&2
          fi

      - name: Diagnostics (on failure)
        if: ${{ failure() }}
        env:
          ENVIRONMENT: ${{ inputs.environment }}
          EC2_HOST: ${{ inputs.ec2_host || secrets.EC2_HOST }}
          SSH_USER: ${{ steps.ssh_preflight.outputs.ssh_user }}
        run: |
          set -euo pipefail
          USER="${SSH_USER:-ec2-user}"
          {
            printf '%s\n' 'set -euo pipefail'
            printf '%s\n' "cd /home/${USER}/bb-portfolio"
            printf '%s\n' 'COMPOSE_FILE="deploy/compose/docker-compose.yml"'
            printf '%s\n' 'echo "=== docker compose ps (all) ==="'
            printf '%s\n' 'docker-compose -f "$COMPOSE_FILE" ps || true'
            printf '%s\n' 'echo "=== docker ps (table) ==="'
            printf '%s\n' 'docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}" || true'
            printf '%s\n' 'echo "=== listeners on :80 and :443 ==="'
            printf '%s\n' "sudo ss -ltnp | egrep ':80|:443' || true"
            printf '%s\n' 'echo "=== curl http://127.0.0.1/ (Host: bbaysinger.com) ==="'
            printf '%s\n' 'curl -I --max-time 8 -H "Host: bbaysinger.com" http://127.0.0.1/ || true'
            printf '%s\n' 'echo "=== curl https://127.0.0.1/ (-k) ==="'
            printf '%s\n' 'curl -kI --max-time 8 https://127.0.0.1/ || true'
            printf '%s\n' 'echo "=== recent logs for likely proxy containers (caddy/nginx) ==="'
            printf '%s\n' "for n in $(docker ps --format '{{.Names}}' | egrep 'caddy|nginx' || true); do echo '--- '"
            printf '%s\n' "echo "'"\$n"'"; docker logs --tail=200 "'"\$n"'" || true; done"
          } | ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null "${USER}@$EC2_HOST" "bash -s"
