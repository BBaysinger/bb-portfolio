# Promote (Blue/Green) Workflow
# GitHub Actions workflow for promoting a candidate EC2 instance to active via EIP handover
# -----------------------------------------------------------------------------------------
# This workflow executes the blue-green deployment promotion process with pre/post
# health checks, optional rollback, and instance retention management.
#
# Role in blue-green deployment:
# - Validates candidate instance health (AWS status + frontend/backend HTTP checks)
# - Re-associates production Elastic IP from active to candidate instance
# - Swaps security groups (candidate gets prod SG, demoted active gets candidate SG)
# - Updates EC2 tags (Role: candidate→active, active→previous)
# - Post-swap health validation with optional automatic rollback
# - Optional: snapshot previous active instance root volume before demotion
# - Optional: immediately terminate previous active instance (cost optimization)
# - Optional: run retention policy to prune old previous instances
#
# Typical promotion workflow:
# 1. Candidate deployed via: deployment-orchestrator.sh --target candidate --profiles prod
# 2. Manual validation: ssh to candidate, test endpoints, verify functionality
# 3. Trigger this workflow manually (Actions tab) with desired health check parameters
# 4. Script performs pre-swap health checks on candidate (fail-fast if unhealthy)
# 5. Confirmation prompt (bypassed via --auto-promote in orchestrator automation)
# 6. EIP handover: production traffic cutover to candidate (typically <1 second)
# 7. Security group swap: new active gets prod SG, demoted gets candidate SG
# 8. Post-swap health validation on new active instance
# 9. Optional rollback if post-swap health fails (re-associate EIP to previous active)
# 10. Optional cleanup: terminate or retain previous active instance per policy
#
# Safety features:
# - Pre-swap health gate prevents promoting unhealthy candidates
# - Configurable retry/interval for transient health check failures
# - Dry-run mode shows planned actions without executing
# - Rollback capability if post-swap health fails
# - Separate security groups isolate candidate during validation
# - Retention policy prevents accidental termination of recent instances
#
# Health checks performed:
# - AWS instance status: both system & instance checks passing (2/2)
# - Frontend: HTTP 200 on :3000 (root path)
# - Backend: HTTP 200 on :3001/api/health
#
# Invocation methods:
# 1. Manual trigger (GitHub Actions UI) - interactive parameter selection
# 2. Via orchestrator: deployment-orchestrator.sh --target candidate --auto-promote
# 3. Via script: scripts/eip-handover.sh --region us-west-2 --auto-promote
# 4. Via gh CLI: gh workflow run promote.yml -f region=us-west-2 -f rollback_on_fail=true
#
# Related scripts:
# - scripts/eip-handover.sh: Core promotion logic with health checks
# - scripts/instance-retention.sh: Retention policy for previous instances
# - deploy/scripts/deployment-orchestrator.sh: Full deployment + optional promotion
#
name: Promote (Blue/Green)

on:
  workflow_dispatch:
    inputs:
      region:
        description: AWS region
        type: string
        required: true
        default: us-west-2
      rollback_on_fail:
        description: Rollback if post-swap health fails
        type: choice
        required: true
        default: "true"
        options: ["true", "false"]
      snapshot_before:
        description: Snapshot active root volume before swap
        type: choice
        required: true
        default: "true"
        options: ["true", "false"]
      max_retries:
        description: Health retries (pre/post)
        type: number
        required: true
        default: 12
      interval:
        description: Seconds between retries
        type: number
        required: true
        default: 10
      dry_run:
        description: Check health only (no swap/tagging)
        type: choice
        required: true
        default: "false"
        options: ["true", "false"]
      destroy_previous:
        description: Immediately terminate previous active (cost optimization)
        type: choice
        required: true
        default: "true"
        options: ["true", "false"]
      prune_after_policy:
        description: After successful promotion, run retention policy (conditional prune of older previous instances)
        type: choice
        required: true
        default: "false"
        options: ["true", "false"]
      retain_count:
        description: Retain N previous instances (when pruning)
        type: number
        required: true
        default: 3
      retain_days:
        description: Minimum age in days before termination (when pruning)
        type: number
        required: false

jobs:
  promote:
    name: Promote candidate via EIP handover
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Prepare SSH key
        id: ssh
        env:
          SSH_KEY_PEM: ${{ secrets.EC2_SSH_KEY_PEM }}
          SSH_KEY_ALT: ${{ secrets.EC2_SSH_KEY }}
        run: |
          set -euo pipefail
          CONTENT="${SSH_KEY_PEM:-}"
          if [ -z "$CONTENT" ] && [ -n "${SSH_KEY_ALT:-}" ]; then
            CONTENT="$SSH_KEY_ALT"
          fi
          if [ -z "$CONTENT" ]; then
            echo "Missing SSH key secret: provide EC2_SSH_KEY_PEM or EC2_SSH_KEY" >&2
            exit 1
          fi
          umask 177
          printf "%s" "$CONTENT" > key.pem
          echo "key=key.pem" >> "$GITHUB_OUTPUT"

      - name: Validate inputs/secrets
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          : "${EC2_HOST}"

      - name: Upload promotion scripts to EC2
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
        run: |
          set -euo pipefail
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" "mkdir -p /home/ec2-user/bb-portfolio/scripts"
          scp -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \
            scripts/eip-handover.sh scripts/instance-retention.sh \
            ec2-user@"$EC2_HOST":/home/ec2-user/bb-portfolio/scripts/
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" "chmod +x /home/ec2-user/bb-portfolio/scripts/*.sh"

      - name: Run eip-handover (health-only or promote)
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          REGION: ${{ inputs.region }}
          ROLLBACK: ${{ inputs.rollback_on_fail }}
          SNAPSHOT: ${{ inputs.snapshot_before }}
          MAX_RETRIES: ${{ inputs.max_retries }}
          INTERVAL: ${{ inputs.interval }}
          DRY_RUN: ${{ inputs.dry_run }}
        run: |
          set -euo pipefail
          flags=("--region" "$REGION" "--max-retries" "$MAX_RETRIES" "--interval" "$INTERVAL")
          if [ "$DRY_RUN" = "true" ]; then : ; else flags+=("--promote"); fi
          if [ "$ROLLBACK" = "true" ]; then flags+=("--rollback-on-fail"); fi
          if [ "$SNAPSHOT" = "true" ]; then flags+=("--snapshot-before"); fi
          printf 'Running: eip-handover.sh %q\n' "${flags[@]}"
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" \
            "cd /home/ec2-user/bb-portfolio && ./scripts/eip-handover.sh ${flags[*]}"

      - name: Retention prune (conditional policy)
        if: ${{ (inputs.prune_after_policy == 'true' || inputs.prune_after_policy == true) && (inputs.destroy_previous != 'true') }}
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          REGION: ${{ inputs.region }}
          RETAIN_COUNT: ${{ inputs.retain_count }}
          RETAIN_DAYS: ${{ inputs.retain_days }}
        run: |
          set -euo pipefail
          flags=("--region" "$REGION" "--retain-count" "$RETAIN_COUNT" "--force")
          if [ -n "${RETAIN_DAYS}" ]; then flags+=("--retain-days" "$RETAIN_DAYS"); fi
          flags+=("--snapshot-before")
          printf 'Running: instance-retention.sh %q\n' "${flags[@]}"
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" \
            "cd /home/ec2-user/bb-portfolio && ./scripts/instance-retention.sh ${flags[*]}"

      - name: Destroy previous instance immediately (override; skips policy)
        if: ${{ inputs.destroy_previous == 'true' || inputs.destroy_previous == true }}
        env:
          EC2_HOST: ${{ secrets.EC2_HOST }}
          REGION: ${{ inputs.region }}
          SNAPSHOT: ${{ inputs.snapshot_before }}
        run: |
          set -euo pipefail
          if [ "${{ inputs.dry_run }}" = "true" ]; then
            echo "Dry-run: would invoke retention with retain-count=0"
            exit 0
          fi
          flags=("--region" "$REGION" "--retain-count" "0" "--force")
          # Use snapshot-before if snapshot flag set for parity with promotion
          if [ "$SNAPSHOT" = "true" ]; then flags+=("--snapshot-before"); fi
          echo "Running immediate previous destroy via instance-retention: ${flags[*]}"
          ssh -i "${{ steps.ssh.outputs.key }}" -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ec2-user@"$EC2_HOST" \
            "cd /home/ec2-user/bb-portfolio && ./scripts/instance-retention.sh ${flags[*]}"
